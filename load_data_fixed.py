# load_data_fixed.py
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ ClickHouse
–£—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤–∞—à–µ–≥–æ CSV
"""

import pandas as pd
from clickhouse_driver import Client
from datetime import datetime
import sys
from pathlib import Path


def load_csv_to_clickhouse(csv_file='data_100k.csv'):
    """–ó–∞–≥—Ä—É–∑–∫–∞ CSV –≤ ClickHouse —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –º–∞–ø–ø–∏–Ω–≥–æ–º –∫–æ–ª–æ–Ω–æ–∫"""

    print(f"\n{'=' * 60}")
    print("–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –í CLICKHOUSE")
    print('=' * 60)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª
    if not Path(csv_file).exists():
        print(f"‚ùå –§–∞–π–ª {csv_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return False

    print(f"üìÅ –§–∞–π–ª: {csv_file}")
    file_size = Path(csv_file).stat().st_size / 1024 / 1024
    print(f"üìè –†–∞–∑–º–µ—Ä: {file_size:.2f} MB")

    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
    print("\nüîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse...")
    try:
        client = Client(
            host='localhost',
            port=9000,
            user='analyst',
            password='admin123'
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        client.execute('SELECT 1')
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
        print("\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ Docker –∑–∞–ø—É—â–µ–Ω:")
        print("  docker-compose up -d")
        return False

    # –°–æ–∑–¥–∞–Ω–∏–µ –ë–î
    print("\nüìù –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
    try:
        client.execute('CREATE DATABASE IF NOT EXISTS card_analytics')
        client.execute('USE card_analytics')
        print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –≥–æ—Ç–æ–≤–∞")
    except Exception as e:
        print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {e}")

    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é —Ç–∞–±–ª–∏—Ü—É –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã
    print("\nüóëÔ∏è –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ç–∞–±–ª–∏—Ü...")
    try:
        client.execute('DROP TABLE IF EXISTS card_analytics.transactions_main')
        print("‚úÖ –°—Ç–∞—Ä–∞—è —Ç–∞–±–ª–∏—Ü–∞ —É–¥–∞–ª–µ–Ω–∞")
    except:
        pass

    # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É CSV —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    print("\nüìä –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã CSV...")
    df_sample = pd.read_csv(csv_file, nrows=5)
    csv_columns = list(df_sample.columns)
    print(f"  –ù–∞–π–¥–µ–Ω–æ {len(csv_columns)} –∫–æ–ª–æ–Ω–æ–∫ –≤ CSV")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    if 'hour' in csv_columns and 'hour_num' not in csv_columns:
        print("  ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ 'hour' - –±—É–¥–µ—Ç –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –≤ 'hour_num'")
    if 'minute' in csv_columns and 'minute_num' not in csv_columns:
        print("  ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ 'minute' - –±—É–¥–µ—Ç –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –≤ 'minute_num'")

    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã CSV
    print("\nüìù –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã...")

    create_table = """
    CREATE TABLE IF NOT EXISTS transactions_main
    (
        hpan Float64,
        transaction_code String,
        rday UInt32,
        day_type String,
        issue_method String,
        card_product_type String,
        emitent_filial String,
        product_category String,
        emitent_region String,
        reissuing_flag String,
        expire_date String,
        issue_date String,
        card_type String,
        product_type String,
        card_bo_table String,
        pinfl String,
        pinfl_flag Nullable(Float32),
        gender String,
        birth_year String,
        age String,
        age_group String,
        iss_flag UInt8,
        emitent_net String,
        emitent_bank String,
        acq_flag UInt8,
        acquirer_net String,
        acquirer_bank String,
        mcc UInt16,
        acquirer_mfo String,
        merchant_name String,
        acquirer_branch String,
        acquirer_region String,
        merchant_type String,
        ip String,
        oked Nullable(Float32),
        login_category String,
        login_group String,
        login String,
        amount_uzs UInt64,
        record_state String,
        reqamt UInt64,
        conamt UInt64,
        match_num UInt32,
        reversal_flag UInt8,
        fe_trace UInt32,
        refnum UInt64,
        sttl_date UInt32,
        sttl_hour UInt8,
        sttl_minute UInt8,
        hour_num UInt8,
        minute_num UInt8,
        udatetime_month UInt8,
        merchant UInt32,
        respcode Nullable(Float32),
        terminal_id String,
        address_name String,
        address_country String,
        currency UInt16,
        merch_id UInt32,
        terminal_type String,
        credit_debit String,
        inst_id UInt32,
        inst_id2 UInt32,
        term_id_key String,
        bo_table String,
        data_flag String,
        trans_type_by_day_key String,
        emission_country String,
        sender_hpan String,
        sender_bank String,
        receiver_hpan String,
        receiver_bank String,
        p2p_flag UInt8,
        p2p_type String
    )
    ENGINE = MergeTree()
    ORDER BY (rday, transaction_code)
    SETTINGS index_granularity = 8192
    """

    try:
        client.execute(create_table)
        print("‚úÖ –¢–∞–±–ª–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞")
    except Exception as e:
        print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü—ã: {e}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV...")

    try:
        # –ß–∏—Ç–∞–µ–º CSV –ø–æ–ª–Ω–æ—Å—Ç—å—é
        df = pd.read_csv(csv_file, low_memory=False)
        print(f"  –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {len(df):,} —Å—Ç—Ä–æ–∫")

        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        rename_map = {}
        if 'hour' in df.columns and 'hour_num' not in df.columns:
            rename_map['hour'] = 'hour_num'
        if 'minute' in df.columns and 'minute_num' not in df.columns:
            rename_map['minute'] = 'minute_num'

        if rename_map:
            print(f"  –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫: {rename_map}")
            df = df.rename(columns=rename_map)

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("  –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

        # –°–ø–∏—Å–æ–∫ –æ–∂–∏–¥–∞–µ–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ
        expected_columns = [
            'hpan', 'transaction_code', 'rday', 'day_type', 'issue_method',
            'card_product_type', 'emitent_filial', 'product_category', 'emitent_region',
            'reissuing_flag', 'expire_date', 'issue_date', 'card_type', 'product_type',
            'card_bo_table', 'pinfl', 'pinfl_flag', 'gender', 'birth_year', 'age',
            'age_group', 'iss_flag', 'emitent_net', 'emitent_bank', 'acq_flag',
            'acquirer_net', 'acquirer_bank', 'mcc', 'acquirer_mfo', 'merchant_name',
            'acquirer_branch', 'acquirer_region', 'merchant_type', 'ip', 'oked',
            'login_category', 'login_group', 'login', 'amount_uzs', 'record_state',
            'reqamt', 'conamt', 'match_num', 'reversal_flag', 'fe_trace', 'refnum',
            'sttl_date', 'sttl_hour', 'sttl_minute', 'hour_num', 'minute_num',
            'udatetime_month', 'merchant', 'respcode', 'terminal_id', 'address_name',
            'address_country', 'currency', 'merch_id', 'terminal_type', 'credit_debit',
            'inst_id', 'inst_id2', 'term_id_key', 'bo_table', 'data_flag',
            'trans_type_by_day_key', 'emission_country', 'sender_hpan', 'sender_bank',
            'receiver_hpan', 'receiver_bank', 'p2p_flag', 'p2p_type'
        ]

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        for col in expected_columns:
            if col not in df.columns:
                print(f"  –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–æ–Ω–∫–∏: {col}")
                df[col] = 0 if col in ['rday', 'mcc', 'amount_uzs'] else ''

        # –ß–∏—Å–ª–æ–≤—ã–µ –ø–æ–ª—è
        numeric_fields = {
            'hpan': 0.0,
            'rday': 0,
            'iss_flag': 0,
            'acq_flag': 0,
            'mcc': 0,
            'amount_uzs': 0,
            'reqamt': 0,
            'conamt': 0,
            'match_num': 0,
            'reversal_flag': 0,
            'fe_trace': 0,
            'refnum': 0,
            'sttl_date': 0,
            'sttl_hour': 0,
            'sttl_minute': 0,
            'hour_num': 0,
            'minute_num': 0,
            'udatetime_month': 0,
            'merchant': 0,
            'currency': 860,
            'merch_id': 0,
            'inst_id': 0,
            'inst_id2': 0,
            'p2p_flag': 0
        }

        for field, default in numeric_fields.items():
            if field in df.columns:
                df[field] = pd.to_numeric(df[field], errors='coerce').fillna(default)

        # Nullable float –ø–æ–ª—è
        nullable_float_fields = ['pinfl_flag', 'oked', 'respcode']
        for field in nullable_float_fields:
            if field in df.columns:
                df[field] = pd.to_numeric(df[field], errors='coerce')
                # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ None –¥–ª—è nullable –ø–æ–ª–µ–π
                df[field] = df[field].where(pd.notnull(df[field]), None)

        # –°—Ç—Ä–æ–∫–æ–≤—ã–µ –ø–æ–ª—è
        string_fields = [col for col in expected_columns if
                         col not in numeric_fields and col not in nullable_float_fields]
        for field in string_fields:
            if field in df.columns:
                df[field] = df[field].fillna('').astype(str)

        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        df = df[expected_columns]

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞—Ç—á–∞–º–∏
        batch_size = 5000  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        total_loaded = 0

        print(f"  –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞–º–∏ –ø–æ {batch_size:,} –∑–∞–ø–∏—Å–µ–π...")

        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i + batch_size]

            try:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DataFrame –≤ —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π
                data = []
                for _, row in batch.iterrows():
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ –∫–æ—Ä—Ç–µ–∂, –∑–∞–º–µ–Ω—è—è NaN –Ω–∞ None
                    row_data = []
                    for val in row.values:
                        if pd.isna(val):
                            row_data.append(None)
                        else:
                            row_data.append(val)
                    data.append(tuple(row_data))

                # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ ClickHouse
                client.execute(
                    'INSERT INTO card_analytics.transactions_main VALUES',
                    data
                )

                total_loaded += len(batch)
                print(f"  ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {total_loaded:,} / {len(df):,}")

            except Exception as e:
                print(f"  ‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –±–∞—Ç—á–∞ {i // batch_size + 1}: {str(e)[:100]}")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–æ —Å–ª–µ–¥—É—é—â–∏–º –±–∞—Ç—á–µ–º
                continue

        print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {total_loaded:,} –∑–∞–ø–∏—Å–µ–π!")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        try:
            count = client.execute('SELECT count() FROM card_analytics.transactions_main')[0][0]
            print(f"\nüìä –ü—Ä–æ–≤–µ—Ä–∫–∞: –≤ —Ç–∞–±–ª–∏—Ü–µ {count:,} –∑–∞–ø–∏—Å–µ–π")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
            print("\nüìà –ü—Ä–∏–º–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")

            # –¢–æ–ø MCC
            mcc_stats = client.execute('''
                SELECT mcc, count() as cnt 
                FROM card_analytics.transactions_main 
                GROUP BY mcc 
                ORDER BY cnt DESC 
                LIMIT 5
            ''')

            if mcc_stats:
                print("\n–¢–æ–ø-5 MCC –∫–∞—Ç–µ–≥–æ—Ä–∏–π:")
                for mcc, cnt in mcc_stats:
                    print(f"  MCC {mcc}: {cnt:,} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")

            # P2P —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            p2p_stats = client.execute('''
                SELECT p2p_flag, count() as cnt, avg(amount_uzs) as avg_amount
                FROM card_analytics.transactions_main
                GROUP BY p2p_flag
            ''')

            if p2p_stats:
                print("\nP2P —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
                for flag, cnt, avg_amt in p2p_stats:
                    p2p_type = "P2P –ø–µ—Ä–µ–≤–æ–¥—ã" if flag == 1 else "–û–±—ã—á–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"
                    print(f"  {p2p_type}: {cnt:,} ({avg_amt:,.0f} UZS —Å—Ä–µ–¥–Ω—è—è —Å—É–º–º–∞)")

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ: {e}")

        print("\n" + "=" * 60)
        print("–ì–û–¢–û–í–û!")
        print("=" * 60)
        print("\nüéâ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ:")
        print("  1. –û—Ç–∫—Ä—ã—Ç—å ClickHouse UI: http://localhost:8123/play")
        print("  2. –ó–∞–ø—É—Å—Ç–∏—Ç—å Streamlit: streamlit run run_app.py")
        print("  3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SQL –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

        return True

    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    csv_file = 'data_100k.csv'

    if len(sys.argv) > 1:
        csv_file = sys.argv[1]

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É
    success = load_csv_to_clickhouse(csv_file)

    if not success:
        sys.exit(1)