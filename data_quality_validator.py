#!/usr/bin/env python3
"""
Data Quality Validation —Å Great Expectations –∏ Pandera
–°–æ–∑–¥–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö
"""

import pandas as pd
import pandera as pa
from pandera import Column, DataFrameSchema, Check
from clickhouse_driver import Client
import json
from datetime import datetime
import logging
import numpy as np

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DataQualityValidator:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π"""

    def __init__(self):
        self.client = Client(
            host='localhost',
            port=9000,
            user='analyst',
            password='admin123',
            database='card_analytics'
        )
        self.validation_results = {}

    def create_transaction_schema(self):
        """–°–æ–∑–¥–∞–µ–º —Å—Ö–µ–º—É –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π —Å Pandera"""

        schema = DataFrameSchema({
            'hpan': Column(str, nullable=False),
            'transaction_code': Column(str, nullable=False, unique=True),
            'amount_uzs': Column(float, Check.ge(0), nullable=False),
            'mcc': Column(int, Check.in_range(0, 9999), nullable=True),
            'p2p_flag': Column(int, Check.isin([0, 1])),
            'hour_num': Column(int, Check.in_range(0, 23)),
            'merchant_name': Column(str, nullable=True),
            'emitent_bank': Column(str, nullable=False),
            'age': Column(str, nullable=True),
            'gender': Column(str, Check.isin(['–ú', '–ñ', '']), nullable=True),
            'respcode': Column(str, nullable=True),
            'reversal_flag': Column(str, nullable=True)
        })

        return schema

    def run_custom_validation(self, df: pd.DataFrame):
        """–ó–∞–ø—É—Å–∫ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫ –≤–º–µ—Å—Ç–æ Great Expectations"""

        logger.info("–ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö...")

        validation_results = {
            'completeness': {},
            'consistency': {},
            'uniqueness': {},
            'validity': {},
            'accuracy': {}
        }

        # COMPLETENESS –ø—Ä–æ–≤–µ—Ä–∫–∏
        logger.info("1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö (Completeness)...")

        critical_columns = ['hpan', 'transaction_code', 'amount_uzs', 'emitent_bank']
        for col in critical_columns:
            if col in df.columns:
                null_count = df[col].isna().sum()
                empty_count = (df[col] == '').sum() if df[col].dtype == 'object' else 0
                missing_count = null_count + empty_count
                missing_percent = (missing_count / len(df)) * 100

                validation_results['completeness'][col] = {
                    'success': missing_percent < 5,  # –°—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º –µ—Å–ª–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤ < 5%
                    'missing_count': missing_count,
                    'missing_percent': round(missing_percent, 2)
                }

        # CONSISTENCY –ø—Ä–æ–≤–µ—Ä–∫–∏
        logger.info("2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ (Consistency)...")

        # –î–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏
        if 'issue_date' in df.columns and 'expire_date' in df.columns:
            try:
                issue_dates = pd.to_datetime(df['issue_date'], errors='coerce')
                expire_dates = pd.to_datetime(df['expire_date'], errors='coerce')
                valid_dates = (expire_dates > issue_dates).fillna(False)
                validation_results['consistency']['dates_order'] = valid_dates.mean() > 0.95
            except:
                validation_results['consistency']['dates_order'] = False

        # –°—É–º–º—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã
        if all(col in df.columns for col in ['reqamt', 'conamt', 'amount_uzs']):
            try:
                reqamt = pd.to_numeric(df['reqamt'], errors='coerce').fillna(0)
                amount_uzs = pd.to_numeric(df['amount_uzs'], errors='coerce').fillna(0)
                amounts_consistent = (amount_uzs <= reqamt * 1.1) | (reqamt == 0)
                validation_results['consistency']['amounts'] = amounts_consistent.mean()
            except:
                validation_results['consistency']['amounts'] = 0

        # UNIQUENESS –ø—Ä–æ–≤–µ—Ä–∫–∏
        logger.info("3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ (Uniqueness)...")

        if 'transaction_code' in df.columns:
            duplicates = df['transaction_code'].duplicated().sum()
            validation_results['uniqueness']['transaction_code'] = {
                'success': duplicates == 0,
                'duplicates': duplicates,
                'duplicate_percent': round((duplicates / len(df)) * 100, 2)
            }

        # VALIDITY –ø—Ä–æ–≤–µ—Ä–∫–∏
        logger.info("4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ (Validity)...")

        # MCC –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        if 'mcc' in df.columns:
            mcc_numeric = pd.to_numeric(df['mcc'], errors='coerce')
            valid_mcc = ((mcc_numeric >= 0) & (mcc_numeric <= 9999)).fillna(False)
            validation_results['validity']['mcc_range'] = valid_mcc.mean() > 0.95

        # –í–æ–∑—Ä–∞—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–∑—É–º–Ω—ã–º
        if 'age' in df.columns:
            age_numeric = pd.to_numeric(df['age'], errors='coerce')
            valid_age = ((age_numeric >= 18) & (age_numeric <= 100)).fillna(False).mean()
            validation_results['validity']['age_range'] = valid_age

        # Hour –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-23
        if 'hour_num' in df.columns:
            hour_numeric = pd.to_numeric(df['hour_num'], errors='coerce')
            valid_hours = ((hour_numeric >= 0) & (hour_numeric <= 23)).fillna(False).mean()
            validation_results['validity']['hour_range'] = valid_hours

        # ACCURACY –ø—Ä–æ–≤–µ—Ä–∫–∏
        logger.info("5. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ (Accuracy)...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É–º–º –Ω–∞ –≤—ã–±—Ä–æ—Å—ã
        if 'amount_uzs' in df.columns:
            amount_numeric = pd.to_numeric(df['amount_uzs'], errors='coerce').dropna()
            if len(amount_numeric) > 0:
                q1 = amount_numeric.quantile(0.25)
                q3 = amount_numeric.quantile(0.75)
                iqr = q3 - q1
                outliers = ((amount_numeric < (q1 - 3 * iqr)) |
                            (amount_numeric > (q3 + 3 * iqr))).sum()

                validation_results['accuracy']['outliers'] = {
                    'count': int(outliers),
                    'percent': round((outliers / len(amount_numeric)) * 100, 2)
                }

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å—É–º–º—ã
        if 'amount_uzs' in df.columns:
            amount_numeric = pd.to_numeric(df['amount_uzs'], errors='coerce')
            negative_amounts = (amount_numeric < 0).sum()
            validation_results['accuracy']['negative_amounts'] = {
                'count': int(negative_amounts),
                'percent': round((negative_amounts / len(df)) * 100, 2)
            }

        return validation_results

    def analyze_data_patterns(self, df: pd.DataFrame):
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö"""

        patterns = {}

        # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        if 'hour_num' in df.columns:
            hour_numeric = pd.to_numeric(df['hour_num'], errors='coerce')
            if not hour_numeric.isna().all():
                peak_hours = hour_numeric.value_counts().head(3).index.tolist()
                patterns['peak_hours'] = [int(h) for h in peak_hours]

        # –ê–Ω–∞–ª–∏–∑ P2P
        if 'p2p_flag' in df.columns:
            p2p_numeric = pd.to_numeric(df['p2p_flag'], errors='coerce')
            patterns['p2p_ratio'] = round(p2p_numeric.mean() * 100, 2) if not p2p_numeric.isna().all() else 0

        # –ê–Ω–∞–ª–∏–∑ —Ç–æ–ø MCC
        if 'mcc' in df.columns:
            mcc_counts = df['mcc'].value_counts().head(5)
            patterns['top_mcc'] = {str(mcc): int(count) for mcc, count in mcc_counts.items()}

        return patterns

    def generate_quality_report(self, table_name: str = 'transactions_optimized'):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""

        logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Data Quality Report –¥–ª—è {table_name}...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        try:
            table_check = self.client.execute(f"EXISTS TABLE {table_name}")
            if not table_check[0][0]:
                logger.error(f"–¢–∞–±–ª–∏—Ü–∞ {table_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
                alt_tables = ['transactions_simple', 'transactions_main', 'card_transactions']
                for alt_table in alt_tables:
                    if self.client.execute(f"EXISTS TABLE card_analytics.{alt_table}")[0][0]:
                        table_name = f"card_analytics.{alt_table}"
                        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–∞–±–ª–∏—Ü—É {table_name}")
                        break
                else:
                    logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏")
                    return None
        except:
            table_name = f"card_analytics.{table_name}"

        # –ó–∞–≥—Ä—É–∂–∞–µ–º sample –¥–∞–Ω–Ω—ã—Ö
        try:
            df = pd.DataFrame(self.client.execute(f"""
                SELECT *
                FROM {table_name}
                LIMIT 10000
            """))
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None

        if df.empty:
            logger.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return None

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
        columns = [col[0] for col in self.client.execute(f"DESCRIBE {table_name}")]
        df.columns = columns[:len(df.columns)]

        # 1. –ó–∞–ø—É—Å–∫–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é
        validation_results = self.run_custom_validation(df)

        # 2. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∞–±–ª–∏—Ü–µ
        try:
            table_stats = self.client.execute(f"""
                SELECT 
                    count() as total_rows,
                    uniq(hpan) as unique_cards,
                    uniq(transaction_code) as unique_transactions
                FROM {table_name}
            """)[0]

            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–∞—Ç—ã, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            date_range = "N/A"
            try:
                dates = self.client.execute(f"""
                    SELECT 
                        min(transaction_date) as min_date,
                        max(transaction_date) as max_date
                    FROM {table_name}
                """)[0]
                if dates[0] and dates[1]:
                    date_range = f"{dates[0]} to {dates[1]}"
            except:
                pass

        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É: {e}")
            table_stats = [len(df), df['hpan'].nunique() if 'hpan' in df.columns else 0,
                           df['transaction_code'].nunique() if 'transaction_code' in df.columns else 0]
            date_range = "N/A"

        # 3. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        missing_analysis = {}
        for col in df.columns:
            missing_count = df[col].isna().sum()
            if df[col].dtype == 'object':
                missing_count += (df[col] == '').sum()
            missing_percent = (missing_count / len(df)) * 100
            missing_analysis[col] = {
                'missing_count': int(missing_count),
                'missing_percent': round(missing_percent, 2)
            }

        # 4. –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        patterns = self.analyze_data_patterns(df)

        # 5. –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        report = {
            'metadata': {
                'table_name': table_name,
                'report_date': datetime.now().isoformat(),
                'sample_size': len(df),
                'total_rows': int(table_stats[0]),
                'unique_cards': int(table_stats[1]),
                'unique_transactions': int(table_stats[2]),
                'date_range': date_range
            },
            'validation_results': validation_results,
            'missing_analysis': missing_analysis,
            'data_patterns': patterns,
            'quality_scores': self.calculate_quality_scores(validation_results, missing_analysis)
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        with open('data_quality_report.json', 'w') as f:
            json.dump(report, f, indent=2, default=str)

        logger.info("‚úÖ Data Quality Report —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ data_quality_report.json")

        return report

    def calculate_quality_scores(self, validation_results, missing_analysis):
        """–†–∞—Å—á–µ—Ç –æ–±—â–∏—Ö –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""

        # Completeness Score
        critical_cols = ['hpan', 'transaction_code', 'amount_uzs', 'emitent_bank']
        completeness_scores = []
        for col in critical_cols:
            if col in missing_analysis:
                score = max(0, 100 - missing_analysis[col]['missing_percent'])
                completeness_scores.append(score)

        completeness_score = sum(completeness_scores) / len(completeness_scores) if completeness_scores else 0

        # Consistency Score
        consistency_checks = validation_results.get('consistency', {})
        consistency_scores = []
        for k, v in consistency_checks.items():
            if isinstance(v, bool):
                consistency_scores.append(100 if v else 0)
            elif isinstance(v, (int, float)):
                consistency_scores.append(v * 100)

        consistency_score = sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0

        # Uniqueness Score
        uniqueness_checks = validation_results.get('uniqueness', {})
        uniqueness_score = 100
        if 'transaction_code' in uniqueness_checks:
            if uniqueness_checks['transaction_code'].get('success', False):
                uniqueness_score = 100
            else:
                dup_percent = uniqueness_checks['transaction_code'].get('duplicate_percent', 0)
                uniqueness_score = max(0, 100 - dup_percent)

        # Validity Score
        validity_checks = validation_results.get('validity', {})
        validity_scores = []
        for k, v in validity_checks.items():
            if isinstance(v, bool):
                validity_scores.append(100 if v else 0)
            elif isinstance(v, (int, float)):
                validity_scores.append(v * 100)

        validity_score = sum(validity_scores) / len(validity_scores) if validity_scores else 0

        # Accuracy Score
        accuracy_checks = validation_results.get('accuracy', {})
        accuracy_score = 100
        if 'outliers' in accuracy_checks:
            outlier_percent = accuracy_checks['outliers'].get('percent', 0)
            accuracy_score -= min(outlier_percent * 2, 30)  # –ú–∞–∫—Å–∏–º—É–º -30% –∑–∞ –≤—ã–±—Ä–æ—Å—ã
        if 'negative_amounts' in accuracy_checks:
            neg_percent = accuracy_checks['negative_amounts'].get('percent', 0)
            accuracy_score -= neg_percent * 5  # -5% –∑–∞ –∫–∞–∂–¥—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—É–º–º

        # Overall Score
        overall_score = (completeness_score + consistency_score + uniqueness_score +
                         validity_score + accuracy_score) / 5

        return {
            'completeness_score': round(completeness_score, 2),
            'consistency_score': round(consistency_score, 2),
            'uniqueness_score': round(uniqueness_score, 2),
            'validity_score': round(validity_score, 2),
            'accuracy_score': round(accuracy_score, 2),
            'overall_score': round(overall_score, 2),
            'grade': self.get_quality_grade(overall_score)
        }

    def get_quality_grade(self, score):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
        if score >= 95:
            return 'A+ (Excellent)'
        elif score >= 90:
            return 'A (Very Good)'
        elif score >= 80:
            return 'B (Good)'
        elif score >= 70:
            return 'C (Acceptable)'
        elif score >= 60:
            return 'D (Poor)'
        else:
            return 'F (Critical)'

    def print_report_summary(self, report):
        """–í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏ –æ—Ç—á–µ—Ç–∞"""

        print("\n" + "=" * 60)
        print("üìä DATA QUALITY REPORT SUMMARY")
        print("=" * 60)

        metadata = report['metadata']
        print(f"\nüìã –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:")
        print(f"  ‚Ä¢ –¢–∞–±–ª–∏—Ü–∞: {metadata['table_name']}")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {metadata['total_rows']:,}")
        print(f"  ‚Ä¢ –ü–µ—Ä–∏–æ–¥: {metadata['date_range']}")
        print(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç: {metadata['unique_cards']:,}")
        print(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {metadata['unique_transactions']:,}")

        scores = report['quality_scores']
        print(f"\nüìà –û—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:")
        print(f"  ‚Ä¢ –ü–æ–ª–Ω–æ—Ç–∞ (Completeness): {scores['completeness_score']}%")
        print(f"  ‚Ä¢ –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (Consistency): {scores['consistency_score']}%")
        print(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å (Uniqueness): {scores['uniqueness_score']}%")
        print(f"  ‚Ä¢ –í–∞–ª–∏–¥–Ω–æ—Å—Ç—å (Validity): {scores['validity_score']}%")
        print(f"  ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {scores['accuracy_score']}%")
        print(f"\n  üéØ –û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê: {scores['overall_score']}% - {scores['grade']}")

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–∞–Ω–Ω—ã—Ö
        if 'data_patterns' in report:
            patterns = report['data_patterns']
            print(f"\nüìä –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–∞–Ω–Ω—ã—Ö:")
            if 'peak_hours' in patterns:
                print(f"  ‚Ä¢ –ü–∏–∫–æ–≤—ã–µ —á–∞—Å—ã: {patterns['peak_hours']}")
            if 'p2p_ratio' in patterns:
                print(f"  ‚Ä¢ –î–æ–ª—è P2P —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {patterns['p2p_ratio']}%")
            if 'top_mcc' in patterns and patterns['top_mcc']:
                print(f"  ‚Ä¢ –¢–æ–ø MCC –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
                for mcc, count in list(patterns['top_mcc'].items())[:3]:
                    print(f"    - MCC {mcc}: {count:,} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")

        # –¢–æ–ø –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –ø–æ–ª–µ–π
        print(f"\n‚ö†Ô∏è –¢–æ–ø-5 –ø–æ–ª–µ–π —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏:")
        missing = report['missing_analysis']
        sorted_missing = sorted(missing.items(), key=lambda x: x[1]['missing_percent'], reverse=True)[:5]
        for field, stats in sorted_missing:
            if stats['missing_percent'] > 0:
                print(f"  ‚Ä¢ {field}: {stats['missing_percent']}% –ø—Ä–æ–ø—É—â–µ–Ω–æ ({stats['missing_count']:,} –∑–∞–ø–∏—Å–µ–π)")

        # –ü—Ä–æ–±–ª–µ–º—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        validation = report['validation_results']
        print(f"\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")

        # Completeness
        completeness = validation.get('completeness', {})
        failed_completeness = [k for k, v in completeness.items() if not v.get('success', True)]
        if failed_completeness:
            print(f"  ‚Ä¢ –ù–µ–ø–æ–ª–Ω—ã–µ –ø–æ–ª—è: {', '.join(failed_completeness)}")

        # Uniqueness
        uniqueness = validation.get('uniqueness', {})
        if 'transaction_code' in uniqueness and uniqueness['transaction_code'].get('duplicates', 0) > 0:
            print(f"  ‚Ä¢ –î—É–±–ª–∏–∫–∞—Ç—ã –≤ transaction_code: {uniqueness['transaction_code']['duplicates']:,}")

        # Accuracy
        accuracy = validation.get('accuracy', {})
        if 'outliers' in accuracy and accuracy['outliers']['count'] > 0:
            print(f"  ‚Ä¢ –í—ã–±—Ä–æ—Å—ã –≤ —Å—É–º–º–∞—Ö: {accuracy['outliers']['count']:,} ({accuracy['outliers']['percent']}%)")
        if 'negative_amounts' in accuracy and accuracy['negative_amounts']['count'] > 0:
            print(f"  ‚Ä¢ –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å—É–º–º—ã: {accuracy['negative_amounts']['count']:,}")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        if scores['completeness_score'] < 90:
            print("  ‚Ä¢ –£–ª—É—á—à–∏—Ç—å —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–µ–π")
        if scores['consistency_score'] < 90:
            print("  ‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ —ç—Ç–∞–ø–µ –∑–∞–≥—Ä—É–∑–∫–∏")
        if scores['validity_score'] < 90:
            print("  ‚Ä¢ –í–Ω–µ–¥—Ä–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –∑–Ω–∞—á–µ–Ω–∏–π")
        if scores['accuracy_score'] < 90:
            print("  ‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—ã–±—Ä–æ—Å—ã –∏ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
        if scores['overall_score'] >= 90:
            print("  ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≤—ã—Å–æ–∫–æ–º —É—Ä–æ–≤–Ω–µ! –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π—Ç–µ —Ç–µ–∫—É—â–∏–π —É—Ä–æ–≤–µ–Ω—å.")

        print("\n" + "=" * 60)


def main():
    """–ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""

    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--table", default="transactions_grade_a")
    args = parser.parse_args()

    validator = DataQualityValidator()

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report = validator.generate_quality_report(args.table)

    if report:
        # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
        validator.print_report_summary(report)

        print("\n‚úÖ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤:")
        print("  ‚Ä¢ data_quality_report.json")

        print("\nüéØ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print("  1. –ò–∑—É—á–∏—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç")
        print("  2. –ò—Å–ø—Ä–∞–≤—å—Ç–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã")
        print("  3. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é")
        print("  4. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ –≤ ETL pipeline")


if __name__ == "__main__":
    main()