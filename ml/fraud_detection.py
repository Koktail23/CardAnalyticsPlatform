#!/usr/bin/env python3
"""
Fraud Detection Model –¥–ª—è –∫–∞—Ä—Ç–æ—á–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ card_features –∏ —Å—Ç—Ä–æ–∏—Ç –º–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π
"""

import pandas as pd
import numpy as np
from clickhouse_driver import Client
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import xgboost as xgb
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import logging
from typing import Tuple, Dict, Any

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FraudDetector:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞"""

    def __init__(self):
        self.client = Client(
            host='localhost',
            port=9000,
            user='analyst',
            password='admin123',
            database='card_analytics'
        )
        self.scaler = StandardScaler()
        self.isolation_forest = None
        self.xgb_model = None

    def prepare_transaction_data(self) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π —Å –º–µ—Ç–∫–∞–º–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ñ—Ä–æ–¥–∞"""

        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ñ—Ä–æ–¥–∞
        query = """
        SELECT 
            hpan,
            transaction_code,
            transaction_date,
            hour_num,
            amount_uzs,
            mcc,
            p2p_flag,
            merchant_name,
            emitent_bank,
            emitent_region,
            respcode,
            reversal_flag,
            -- –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ñ—Ä–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª
            CASE 
                WHEN respcode != '' AND respcode != '00' THEN 1  -- –ù–µ—É—Å–ø–µ—à–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
                WHEN reversal_flag = '1' THEN 1  -- –û—Ç–º–µ–Ω–µ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
                WHEN amount_uzs > 50000000 THEN 1  -- –û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ —Å—É–º–º—ã
                WHEN hour_num BETWEEN 2 AND 5 THEN 1  -- –ù–æ—á–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
                ELSE 0
            END as potential_fraud
        FROM transactions_optimized
        WHERE amount_uzs > 0
        """

        df = pd.DataFrame(self.client.execute(query))
        columns = ['hpan', 'transaction_code', 'transaction_date', 'hour_num',
                   'amount_uzs', 'mcc', 'p2p_flag', 'merchant_name',
                   'emitent_bank', 'emitent_region', 'respcode',
                   'reversal_flag', 'potential_fraud']
        df.columns = columns

        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
        logger.info(f"–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ñ—Ä–æ–¥: {df['potential_fraud'].sum()} ({df['potential_fraud'].mean() * 100:.2f}%)")

        return df

    def create_transaction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"""

        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π...")

        # Velocity features - —Å–∫–æ—Ä–æ—Å—Ç—å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
        df = df.sort_values(['hpan', 'transaction_date'])

        # –í—Ä–µ–º—è —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–µ–π
        df['prev_txn_date'] = df.groupby('hpan')['transaction_date'].shift(1)
        df['days_since_prev_txn'] = (df['transaction_date'] - df['prev_txn_date']).dt.days.fillna(999)

        # –°—É–º–º–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        df['prev_amount'] = df.groupby('hpan')['amount_uzs'].shift(1)
        df['amount_change_ratio'] = (df['amount_uzs'] / df['prev_amount'].replace(0, 1)).fillna(1)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ä—Ç–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        df['card_avg_amount'] = df.groupby('hpan')['amount_uzs'].transform(
            lambda x: x.rolling(window=10, min_periods=1).mean()
        )
        df['amount_deviation'] = df['amount_uzs'] / df['card_avg_amount'].replace(0, 1)

        # –ß–∞—Å—Ç–æ—Ç–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –≤ —á–∞—Å
        df['txn_hour_count'] = df.groupby(['hpan', 'transaction_date', 'hour_num']).cumcount() + 1

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ MCC
        df['is_risky_mcc'] = df['mcc'].isin([6010, 6011, 6012, 7995]).astype(int)  # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∏ –∏–≥—Ä–æ–≤—ã–µ MCC

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['is_weekend'] = df['transaction_date'].dt.dayofweek.isin([5, 6]).astype(int)
        df['is_night'] = df['hour_num'].between(22, 6).astype(int)

        # –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['is_capital'] = (df['emitent_region'] == 'Tashkent City').astype(int)

        return df

    def load_card_features(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–∞—Ä—Ç –∏–∑ card_features"""

        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–∞—Ä—Ç...")

        query = """
        SELECT 
            hpan,
            txn_count_30d,
            txn_amount_30d,
            avg_txn_amount_30d,
            p2p_ratio_30d,
            unique_mcc_30d,
            unique_merchants_30d,
            weekend_ratio,
            night_txn_ratio,
            days_since_last_txn,
            customer_segment
        FROM card_features
        """

        df = pd.DataFrame(self.client.execute(query))
        columns = ['hpan', 'txn_count_30d', 'txn_amount_30d', 'avg_txn_amount_30d',
                   'p2p_ratio_30d', 'unique_mcc_30d', 'unique_merchants_30d',
                   'weekend_ratio', 'night_txn_ratio', 'days_since_last_txn',
                   'customer_segment']
        df.columns = columns

        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(df)} –∫–∞—Ä—Ç")

        return df

    def prepare_final_dataset(self) -> Tuple[pd.DataFrame, pd.Series]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        txn_df = self.prepare_transaction_data()
        txn_df = self.create_transaction_features(txn_df)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–∞—Ä—Ç
        card_features = self.load_card_features()

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º
        df = txn_df.merge(card_features, on='hpan', how='left')

        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
        feature_columns = [
            'amount_uzs', 'hour_num', 'p2p_flag',
            'days_since_prev_txn', 'amount_change_ratio', 'amount_deviation',
            'txn_hour_count', 'is_risky_mcc', 'is_weekend', 'is_night', 'is_capital',
            'txn_count_30d', 'avg_txn_amount_30d', 'p2p_ratio_30d',
            'unique_mcc_30d', 'unique_merchants_30d', 'weekend_ratio', 'night_txn_ratio'
        ]

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        df[feature_columns] = df[feature_columns].fillna(0)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–±—Ä–æ—Å—ã
        df['amount_uzs'] = np.clip(df['amount_uzs'], 0, df['amount_uzs'].quantile(0.99))
        df['amount_deviation'] = np.clip(df['amount_deviation'], 0, 10)

        X = df[feature_columns]
        y = df['potential_fraud']

        logger.info(f"–§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {X.shape[0]} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {y.value_counts().to_dict()}")

        return X, y

    def train_isolation_forest(self, X: pd.DataFrame) -> IsolationForest:
        """–û–±—É—á–µ–Ω–∏–µ Isolation Forest –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π"""

        logger.info("–û–±—É—á–µ–Ω–∏–µ Isolation Forest...")

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        X_scaled = self.scaler.fit_transform(X)

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.isolation_forest = IsolationForest(
            contamination=0.01,  # –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π
            random_state=42,
            n_estimators=100
        )

        self.isolation_forest.fit(X_scaled)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = self.isolation_forest.predict(X_scaled)
        anomalies = (predictions == -1).sum()

        logger.info(f"Isolation Forest –æ–±—É—á–µ–Ω. –ù–∞–π–¥–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {anomalies} ({anomalies / len(X) * 100:.2f}%)")

        return self.isolation_forest

    def train_xgboost(self, X: pd.DataFrame, y: pd.Series) -> xgb.XGBClassifier:
        """–û–±—É—á–µ–Ω–∏–µ XGBoost –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""

        logger.info("–û–±—É—á–µ–Ω–∏–µ XGBoost...")

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ –≤–µ—Å–∞
        scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.xgb_model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            scale_pos_weight=scale_pos_weight,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        )

        self.xgb_model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            early_stopping_rounds=10,
            verbose=False
        )

        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        y_pred = self.xgb_model.predict(X_test)
        y_pred_proba = self.xgb_model.predict_proba(X_test)[:, 1]

        # –ú–µ—Ç—Ä–∏–∫–∏
        print("\n" + "=" * 60)
        print("–ú–ï–¢–†–ò–ö–ò XGBOOST")
        print("=" * 60)
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))

        # ROC-AUC
        if len(np.unique(y_test)) > 1:
            roc_auc = roc_auc_score(y_test, y_pred_proba)
            print(f"\nROC-AUC Score: {roc_auc:.4f}")

        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        print(f"\nConfusion Matrix:")
        print(f"TN: {cm[0, 0]:,}  FP: {cm[0, 1]:,}")
        print(f"FN: {cm[1, 0]:,}  TP: {cm[1, 1]:,}")

        return self.xgb_model

    def plot_feature_importance(self, X: pd.DataFrame):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""

        if self.xgb_model is None:
            logger.error("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return

        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        importance = pd.DataFrame({
            'feature': X.columns,
            'importance': self.xgb_model.feature_importances_
        }).sort_values('importance', ascending=False).head(15)

        plt.figure(figsize=(10, 6))
        plt.barh(importance['feature'], importance['importance'])
        plt.xlabel('Importance')
        plt.title('Top 15 Feature Importances')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig('feature_importance.png')
        plt.show()

        print("\n" + "=" * 60)
        print("–¢–û–ü-10 –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í")
        print("=" * 60)
        for idx, row in importance.head(10).iterrows():
            print(f"{row['feature']}: {row['importance']:.4f}")

    def save_models(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""

        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        joblib.dump(self.scaler, 'fraud_scaler.pkl')
        joblib.dump(self.isolation_forest, 'fraud_isolation_forest.pkl')
        joblib.dump(self.xgb_model, 'fraud_xgboost.pkl')

        logger.info("–ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

    def run_full_pipeline(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—É—á–µ–Ω–∏—è"""

        print("\nüöÄ –ó–∞–ø—É—Å–∫ Fraud Detection Pipeline...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y = self.prepare_final_dataset()

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        self.train_isolation_forest(X)
        self.train_xgboost(X, y)

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        self.plot_feature_importance(X)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.save_models()

        print("\n" + "=" * 60)
        print("‚úÖ FRAUD DETECTION PIPELINE –ó–ê–í–ï–†–®–ï–ù!")
        print("=" * 60)


def main():
    detector = FraudDetector()
    detector.run_full_pipeline()

    print("\nüéØ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("  1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —Å–∫–æ—Ä–∏–Ω–≥–∞ –Ω–æ–≤—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
    print("  2. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è –∞–ª–µ—Ä—Ç–æ–≤")
    print("  3. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ –≤ real-time —Å–∏—Å—Ç–µ–º—É")


if __name__ == "__main__":
    main()