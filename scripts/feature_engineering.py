#!/usr/bin/env python3
"""
Feature Engineering –¥–ª—è –∫–∞—Ä—Ç–æ—á–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML –º–æ–¥–µ–ª–µ–π
"""

from clickhouse_driver import Client
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FeatureEngineer:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""

    def __init__(self):
        self.client = Client(
            host='localhost',
            port=9000,
            user='analyst',
            password='admin123',
            database='card_analytics'
        )

    def create_feature_table(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""

        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é —Ç–∞–±–ª–∏—Ü—É –µ—Å–ª–∏ –µ—Å—Ç—å
        self.client.execute("DROP TABLE IF EXISTS card_features")

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        create_sql = """
        CREATE TABLE card_features
        (
            -- –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
            hpan String,
            calculation_date Date,

            -- –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (7, 30, 90 –¥–Ω–µ–π)
            txn_count_7d UInt32,
            txn_count_30d UInt32,
            txn_count_90d UInt32,

            txn_amount_7d Float64,
            txn_amount_30d Float64,
            txn_amount_90d Float64,

            avg_txn_amount_7d Float64,
            avg_txn_amount_30d Float64,
            avg_txn_amount_90d Float64,

            -- P2P –ø—Ä–∏–∑–Ω–∞–∫–∏
            p2p_count_30d UInt32,
            p2p_amount_30d Float64,
            p2p_ratio_30d Float32,

            -- MCC —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
            unique_mcc_7d UInt16,
            unique_mcc_30d UInt16,
            top_mcc UInt16,
            top_mcc_ratio Float32,

            -- Merchant –ø—Ä–∏–∑–Ω–∞–∫–∏
            unique_merchants_30d UInt32,
            top_merchant String,
            top_merchant_ratio Float32,

            -- –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            avg_hour UInt8,
            weekend_ratio Float32,
            night_txn_ratio Float32,

            -- Velocity –ø—Ä–∏–∑–Ω–∞–∫–∏
            max_daily_txn_count UInt32,
            max_daily_amount Float64,
            days_since_last_txn UInt16,

            -- –†–∏—Å–∫ –ø—Ä–∏–∑–Ω–∞–∫–∏
            failed_txn_count UInt32,
            reversal_count UInt32,
            international_txn_count UInt32,

            -- –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
            customer_segment String,
            activity_level String,

            created_at DateTime DEFAULT now()
        )
        ENGINE = MergeTree()
        ORDER BY (hpan, calculation_date)
        PARTITION BY toYYYYMM(calculation_date)
        """

        self.client.execute(create_sql)
        logger.info("‚úÖ –¢–∞–±–ª–∏—Ü–∞ card_features —Å–æ–∑–¥–∞–Ω–∞")

    def calculate_transaction_features(self) -> pd.DataFrame:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""

        logger.info("–†–∞—Å—á–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫–∏–µ –¥–∞—Ç—ã –µ—Å—Ç—å –≤ —Ç–∞–±–ª–∏—Ü–µ
        date_check = self.client.execute("""
            SELECT 
                min(transaction_date) as min_date,
                max(transaction_date) as max_date,
                count() as total
            FROM transactions_optimized
        """)[0]

        logger.info(f"–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {date_check[0]} - {date_check[1]}, –≤—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {date_check[2]}")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–∞—Ç—É –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        max_date = date_check[1]

        # –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∏–ª–∏ —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä–∞—è/–Ω–æ–≤–∞—è
        if max_date is None or max_date.year < 2020 or max_date.year > 2030:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–∞—Ç—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            max_date = datetime(2025, 4, 13).date()
            logger.warning(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–∞—Ç–∞: {max_date}")

        logger.info(f"–ë–∞–∑–æ–≤–∞—è –¥–∞—Ç–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–æ–≤: {max_date}")

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç—ã
        features_query = f"""
        SELECT
            hpan,
            toDate('{max_date}') as calculation_date,

            -- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∑–∞ –ø–µ—Ä–∏–æ–¥—ã
            countIf(transaction_date >= toDate('{max_date}') - INTERVAL 7 DAY) as txn_count_7d,
            countIf(transaction_date >= toDate('{max_date}') - INTERVAL 30 DAY) as txn_count_30d,
            countIf(transaction_date >= toDate('{max_date}') - INTERVAL 90 DAY) as txn_count_90d,

            -- –°—É–º–º—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
            sumIf(amount_uzs, transaction_date >= toDate('{max_date}') - INTERVAL 7 DAY) as txn_amount_7d,
            sumIf(amount_uzs, transaction_date >= toDate('{max_date}') - INTERVAL 30 DAY) as txn_amount_30d,
            sumIf(amount_uzs, transaction_date >= toDate('{max_date}') - INTERVAL 90 DAY) as txn_amount_90d,

            -- –°—Ä–µ–¥–Ω–∏–µ —Å—É–º–º—ã
            avgIf(amount_uzs, transaction_date >= toDate('{max_date}') - INTERVAL 7 DAY) as avg_txn_amount_7d,
            avgIf(amount_uzs, transaction_date >= toDate('{max_date}') - INTERVAL 30 DAY) as avg_txn_amount_30d,
            avgIf(amount_uzs, transaction_date >= toDate('{max_date}') - INTERVAL 90 DAY) as avg_txn_amount_90d,

            -- P2P –º–µ—Ç—Ä–∏–∫–∏
            countIf(p2p_flag = 1 AND transaction_date >= toDate('{max_date}') - INTERVAL 30 DAY) as p2p_count_30d,
            sumIf(amount_uzs, p2p_flag = 1 AND transaction_date >= toDate('{max_date}') - INTERVAL 30 DAY) as p2p_amount_30d,

            -- MCC —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
            uniqIf(mcc, transaction_date >= toDate('{max_date}') - INTERVAL 7 DAY) as unique_mcc_7d,
            uniqIf(mcc, transaction_date >= toDate('{max_date}') - INTERVAL 30 DAY) as unique_mcc_30d,

            -- –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–µ—Ä—á–∞–Ω—Ç—ã
            uniqIf(merchant_name, transaction_date >= toDate('{max_date}') - INTERVAL 30 DAY) as unique_merchants_30d,

            -- –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            avg(hour_num) as avg_hour,
            countIf(toDayOfWeek(transaction_date) IN (6, 7)) / greatest(count(), 1) as weekend_ratio,
            countIf(hour_num >= 22 OR hour_num <= 6) / greatest(count(), 1) as night_txn_ratio,

            -- Velocity (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç)
            count() / greatest(uniq(transaction_date), 1) as avg_daily_txn_count,
            dateDiff('day', max(transaction_date), toDate('{max_date}')) as days_since_last_txn,

            -- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            count() as total_txn_count

        FROM transactions_optimized
        WHERE hpan != ''
            AND transaction_date <= toDate('{max_date}')
            AND transaction_date >= toDate('{max_date}') - INTERVAL 90 DAY
        GROUP BY hpan
        HAVING total_txn_count >= 5
        """

        result = self.client.execute(features_query)

        if not result:
            logger.error("–ó–∞–ø—Ä–æ—Å –Ω–µ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã—Ö! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞—Ç—ã –≤ —Ç–∞–±–ª–∏—Ü–µ transactions_optimized")
            logger.info("–ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –¥–∞—Ç–∞–º...")

            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞—Ç
            features_query = """
            SELECT
                hpan,
                today() as calculation_date,
                count() as txn_count_7d,
                count() as txn_count_30d,
                count() as txn_count_90d,
                sum(amount_uzs) as txn_amount_7d,
                sum(amount_uzs) as txn_amount_30d,
                sum(amount_uzs) as txn_amount_90d,
                avg(amount_uzs) as avg_txn_amount_7d,
                avg(amount_uzs) as avg_txn_amount_30d,
                avg(amount_uzs) as avg_txn_amount_90d,
                countIf(p2p_flag = 1) as p2p_count_30d,
                sumIf(amount_uzs, p2p_flag = 1) as p2p_amount_30d,
                uniq(mcc) as unique_mcc_7d,
                uniq(mcc) as unique_mcc_30d,
                uniq(merchant_name) as unique_merchants_30d,
                avg(hour_num) as avg_hour,
                countIf(toDayOfWeek(transaction_date) IN (6, 7)) / greatest(count(), 1) as weekend_ratio,
                countIf(hour_num >= 22 OR hour_num <= 6) / greatest(count(), 1) as night_txn_ratio,
                count() / 30 as avg_daily_txn_count,
                0 as days_since_last_txn,
                count() as total_txn_count
            FROM transactions_optimized
            WHERE hpan != ''
            GROUP BY hpan
            HAVING total_txn_count >= 5
            """

            result = self.client.execute(features_query)

            if not result:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã transactions_optimized")

        df = pd.DataFrame(result)

        # –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
        columns = [
            'hpan', 'calculation_date',
            'txn_count_7d', 'txn_count_30d', 'txn_count_90d',
            'txn_amount_7d', 'txn_amount_30d', 'txn_amount_90d',
            'avg_txn_amount_7d', 'avg_txn_amount_30d', 'avg_txn_amount_90d',
            'p2p_count_30d', 'p2p_amount_30d',
            'unique_mcc_7d', 'unique_mcc_30d',
            'unique_merchants_30d',
            'avg_hour', 'weekend_ratio', 'night_txn_ratio',
            'avg_daily_txn_count', 'days_since_last_txn',
            'total_txn_count'
        ]
        df.columns = columns

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['p2p_ratio_30d'] = df['p2p_count_30d'] / df['txn_count_30d'].replace(0, 1)
        df['max_daily_txn_count'] = df['avg_daily_txn_count'].round().astype(int)  # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç

        logger.info(f"‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(df)} –∫–∞—Ä—Ç")

        return df

    def calculate_mcc_preferences(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ MCC –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""

        logger.info("–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π MCC...")

        # –¢–æ–ø MCC –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç—ã
        top_mcc_query = """
        WITH mcc_stats AS (
            SELECT 
                hpan,
                mcc,
                count() as mcc_count,
                row_number() OVER (PARTITION BY hpan ORDER BY mcc_count DESC) as rn
            FROM transactions_optimized
            WHERE mcc > 0
            GROUP BY hpan, mcc
        )
        SELECT 
            hpan,
            mcc as top_mcc,
            mcc_count,
            mcc_count / sum(mcc_count) OVER (PARTITION BY hpan) as mcc_ratio
        FROM mcc_stats
        WHERE rn = 1
        """

        mcc_df = pd.DataFrame(self.client.execute(top_mcc_query))
        if not mcc_df.empty:
            mcc_df.columns = ['hpan', 'top_mcc', 'mcc_count', 'top_mcc_ratio']
            df = df.merge(mcc_df[['hpan', 'top_mcc', 'top_mcc_ratio']], on='hpan', how='left')

        return df

    def segment_customers(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è"""

        logger.info("–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤...")

        # RFM-–ø–æ–¥–æ–±–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
        df['recency_score'] = pd.cut(df['days_since_last_txn'],
                                     bins=[0, 7, 30, 90, float('inf')],
                                     labels=['Active', 'Regular', 'Sleeping', 'Lost'])

        df['frequency_score'] = pd.cut(df['txn_count_30d'],
                                       bins=[0, 5, 15, 50, float('inf')],
                                       labels=['Low', 'Medium', 'High', 'VeryHigh'])

        df['monetary_score'] = pd.qcut(df['txn_amount_30d'],
                                       q=4,
                                       labels=['Low', 'Medium', 'High', 'Premium'])

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç
        def get_segment(row):
            if row['recency_score'] == 'Active' and row['monetary_score'] in ['High', 'Premium']:
                return 'Champions'
            elif row['recency_score'] == 'Active' and row['frequency_score'] in ['High', 'VeryHigh']:
                return 'Loyal'
            elif row['recency_score'] == 'Regular':
                return 'Potential'
            elif row['recency_score'] == 'Sleeping':
                return 'At Risk'
            else:
                return 'Lost'

        df['customer_segment'] = df.apply(get_segment, axis=1)

        # –£—Ä–æ–≤–µ–Ω—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        df['activity_level'] = df['frequency_score']

        logger.info(f"‚úÖ –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")

        return df

    def save_features_to_db(self, df: pd.DataFrame):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ ClickHouse"""

        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –ë–î...")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ —Å—Ç—Ä–æ–∫–∏
        for col in df.columns:
            if df[col].dtype.name == 'category':
                df[col] = df[col].astype(str)

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        numeric_columns = df.select_dtypes(include=['float64', 'int64', 'float32', 'int32']).columns
        df[numeric_columns] = df[numeric_columns].fillna(0)

        string_columns = df.select_dtypes(include=['object', 'string']).columns
        df[string_columns] = df[string_columns].fillna('')

        # –ü—Ä–∏–≤–æ–¥–∏–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∫ –Ω—É–∂–Ω—ã–º –¥–ª—è ClickHouse
        df['avg_hour'] = df['avg_hour'].fillna(0).round().astype(int)
        df['max_daily_txn_count'] = df['max_daily_txn_count'].fillna(0).round().astype(int)
        df['days_since_last_txn'] = df['days_since_last_txn'].fillna(0).round().astype(int)
        df['unique_mcc_7d'] = df['unique_mcc_7d'].fillna(0).astype(int)
        df['unique_mcc_30d'] = df['unique_mcc_30d'].fillna(0).astype(int)
        df['unique_merchants_30d'] = df['unique_merchants_30d'].fillna(0).astype(int)

        # –î–ª—è UInt32 –ø–æ–ª–µ–π
        for col in ['txn_count_7d', 'txn_count_30d', 'txn_count_90d', 'p2p_count_30d']:
            if col in df.columns:
                df[col] = df[col].fillna(0).astype(int)

        # –î–ª—è Float –ø–æ–ª–µ–π
        float_cols = ['txn_amount_7d', 'txn_amount_30d', 'txn_amount_90d',
                      'avg_txn_amount_7d', 'avg_txn_amount_30d', 'avg_txn_amount_90d',
                      'p2p_amount_30d', 'weekend_ratio', 'night_txn_ratio', 'p2p_ratio_30d']
        for col in float_cols:
            if col in df.columns:
                df[col] = df[col].fillna(0.0).astype(float)

        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ customer_segment –∏ activity_level - —Å—Ç—Ä–æ–∫–∏
        df['customer_segment'] = df['customer_segment'].astype(str)
        df['activity_level'] = df['activity_level'].astype(str)

        # –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        columns_to_save = [
            'hpan', 'calculation_date',
            'txn_count_7d', 'txn_count_30d', 'txn_count_90d',
            'txn_amount_7d', 'txn_amount_30d', 'txn_amount_90d',
            'avg_txn_amount_7d', 'avg_txn_amount_30d', 'avg_txn_amount_90d',
            'p2p_count_30d', 'p2p_amount_30d', 'p2p_ratio_30d',
            'unique_mcc_7d', 'unique_mcc_30d',
            'unique_merchants_30d',
            'avg_hour', 'weekend_ratio', 'night_txn_ratio',
            'max_daily_txn_count', 'days_since_last_txn',
            'customer_segment', 'activity_level'
        ]

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        for col in columns_to_save:
            if col not in df.columns:
                if col in ['top_mcc', 'unique_mcc_7d', 'unique_mcc_30d']:
                    df[col] = 0
                elif col in ['top_mcc_ratio', 'weekend_ratio', 'night_txn_ratio', 'p2p_ratio_30d']:
                    df[col] = 0.0
                elif col in ['top_merchant', 'customer_segment', 'activity_level']:
                    df[col] = ''
                else:
                    df[col] = 0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –≤—Å—Ç–∞–≤–∫–æ–π
        logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –≤—Å—Ç–∞–≤–∫–æ–π:")
        for col in columns_to_save:
            dtype = df[col].dtype
            null_count = df[col].isna().sum()
            logger.debug(f"  {col}: {dtype}, nulls: {null_count}")

        # –í—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á–∞–º–∏
        batch_size = 1000
        total_rows = len(df)

        for i in range(0, total_rows, batch_size):
            batch = df.iloc[i:i + batch_size]
            data = batch[columns_to_save].values.tolist()

            insert_query = f"""
            INSERT INTO card_features (
                hpan, calculation_date,
                txn_count_7d, txn_count_30d, txn_count_90d,
                txn_amount_7d, txn_amount_30d, txn_amount_90d,
                avg_txn_amount_7d, avg_txn_amount_30d, avg_txn_amount_90d,
                p2p_count_30d, p2p_amount_30d, p2p_ratio_30d,
                unique_mcc_7d, unique_mcc_30d,
                unique_merchants_30d,
                avg_hour, weekend_ratio, night_txn_ratio,
                max_daily_txn_count, days_since_last_txn,
                customer_segment, activity_level
            ) VALUES
            """

            try:
                self.client.execute(insert_query, data)

                if (i + batch_size) % 5000 == 0:
                    logger.info(f"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ {min(i + batch_size, total_rows)}/{total_rows} –∑–∞–ø–∏—Å–µ–π")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ –±–∞—Ç—á–∞ {i // batch_size + 1}: {e}")
                logger.error(f"–ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –±–∞—Ç—á–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏:")
                for j, col in enumerate(columns_to_save):
                    if len(data) > 0:
                        logger.error(f"  {col}: {data[0][j]} (—Ç–∏–ø: {type(data[0][j]).__name__})")
                raise

        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_rows} –∑–∞–ø–∏—Å–µ–π —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")

    def generate_feature_report(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""

        logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞...")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
        stats = self.client.execute("""
            SELECT
                count() as total_cards,
                avg(txn_count_30d) as avg_monthly_txn,
                avg(txn_amount_30d) as avg_monthly_amount,
                avg(p2p_ratio_30d) as avg_p2p_ratio,
                avg(unique_mcc_30d) as avg_unique_mcc,
                avg(unique_merchants_30d) as avg_unique_merchants
            FROM card_features
        """)[0]

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
        segments = self.client.execute("""
            SELECT
                customer_segment,
                count() as cnt,
                avg(txn_amount_30d) as avg_amount
            FROM card_features
            GROUP BY customer_segment
            ORDER BY cnt DESC
        """)

        print("\n" + "=" * 60)
        print("üìä –û–¢–ß–ï–¢ –ü–û FEATURE ENGINEERING")
        print("=" * 60)

        print(f"\nüìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –∫–∞—Ä—Ç —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏: {stats[0]:,}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –≤ –º–µ—Å—è—Ü: {stats[1]:.1f}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –º–µ—Å—è—á–Ω—ã–π –æ–±–æ—Ä–æ—Ç: {stats[2]:,.0f} UZS")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–æ–ª—è P2P: {stats[3]:.1%}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö MCC: {stats[4]:.1f}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Ä—á–∞–Ω—Ç–æ–≤: {stats[5]:.1f}")

        print(f"\nüéØ –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤:")
        for segment, cnt, avg_amount in segments:
            print(f"  ‚Ä¢ {segment}: {cnt:,} –∫–∞—Ä—Ç ({cnt / stats[0] * 100:.1f}%) - {avg_amount:,.0f} UZS/–º–µ—Å")

        print("\n" + "=" * 60)
        print("‚úÖ Feature Engineering –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print("=" * 60)

    def run_full_pipeline(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""

        print("\nüöÄ –ó–∞–ø—É—Å–∫ Feature Engineering Pipeline...")

        # 1. –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
        self.create_feature_table()

        # 2. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df = self.calculate_transaction_features()

        # 3. –î–æ–±–∞–≤–ª—è–µ–º MCC –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
        df = self.calculate_mcc_preferences(df)

        # 4. –°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç–æ–≤
        df = self.segment_customers(df)

        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        self.save_features_to_db(df)

        # 6. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        self.generate_feature_report()

        return df


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""

    engineer = FeatureEngineer()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å
    features_df = engineer.run_full_pipeline()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º sample –≤ CSV –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    features_df.head(1000).to_csv('card_features_sample.csv', index=False)
    print("\nüíæ –ü—Ä–∏–º–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ card_features_sample.csv")

    print("\nüéØ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("  1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è Fraud Detection –º–æ–¥–µ–ª–∏")
    print("  2. –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—ä–µ–º–æ–≤")
    print("  3. –°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤")


if __name__ == "__main__":
    main()