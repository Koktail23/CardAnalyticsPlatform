# scripts/init_database.py
"""
–°–∫—Ä–∏–ø—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö ClickHouse –¥–ª—è Card Analytics Platform
–¢–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
"""

import sys
import os
from pathlib import Path
import time
import logging
from typing import Dict, Any
import pandas as pd
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.clickhouse_client import get_clickhouse_manager

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DatabaseInitializer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ë–î"""

    def __init__(self):
        self.ch_manager = get_clickhouse_manager()
        self.migrations_dir = Path('database/migrations')

    def check_connection(self, max_retries: int = 5) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ ClickHouse —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏

        Args:
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫

        Returns:
            True –µ—Å–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
        """
        logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ ClickHouse...")

        for attempt in range(max_retries):
            try:
                if self.ch_manager.test_connection():
                    logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse —É—Å–ø–µ—à–Ω–æ!")
                    return True
            except Exception as e:
                logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)

        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ ClickHouse")
        return False

    def run_migrations(self) -> bool:
        """
        –ó–∞–ø—É—Å–∫ SQL –º–∏–≥—Ä–∞—Ü–∏–π

        Returns:
            True –µ—Å–ª–∏ –≤—Å–µ –º–∏–≥—Ä–∞—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ
        """
        logger.info("–ó–∞–ø—É—Å–∫ –º–∏–≥—Ä–∞—Ü–∏–π...")

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –º–∏–≥—Ä–∞—Ü–∏–π
        migration_files = sorted(self.migrations_dir.glob('*.sql'))

        if not migration_files:
            logger.warning("–§–∞–π–ª—ã –º–∏–≥—Ä–∞—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –º–∏–≥—Ä–∞—Ü–∏—é
            return self.run_embedded_migration()

        for migration_file in migration_files:
            logger.info(f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –º–∏–≥—Ä–∞—Ü–∏–∏: {migration_file.name}")

            try:
                with open(migration_file, 'r', encoding='utf-8') as f:
                    sql_content = f.read()

                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
                commands = [cmd.strip() for cmd in sql_content.split(';') if cmd.strip()]

                for command in commands:
                    if command:
                        self.ch_manager.execute(command)

                logger.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è {migration_file.name} –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º–∏–≥—Ä–∞—Ü–∏–∏ {migration_file.name}: {e}")
                return False

        return True

    def run_embedded_migration(self) -> bool:
        """–ó–∞–ø—É—Å–∫ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏ –µ—Å–ª–∏ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"""
        logger.info("–ó–∞–ø—É—Å–∫ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏...")

        try:
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            self.ch_manager.execute("CREATE DATABASE IF NOT EXISTS card_analytics")
            self.ch_manager.execute("USE card_analytics")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
            tables = self.ch_manager.query("SHOW TABLES")
            existing_tables = [t[0] for t in tables]

            if 'transactions_main' not in existing_tables:
                logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã transactions_main...")

                # –°–æ–∑–¥–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –¥–ª—è 74 –∫–æ–ª–æ–Ω–æ–∫
                create_table_sql = """
                CREATE TABLE IF NOT EXISTS transactions_main
                (
                    -- –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
                    transaction_code String,
                    rday UInt32,
                    day_type String,

                    -- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–∞—Ä—Ç–µ
                    hpan Float64,
                    card_product_type String,
                    card_type String,
                    product_type String,
                    product_category String,
                    card_bo_table String,
                    issue_method String,
                    issue_date Date,
                    expire_date Date,
                    reissuing_flag String,

                    -- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∏–µ–Ω—Ç–µ
                    pinfl String,
                    pinfl_flag Nullable(Float32),
                    gender String,
                    birth_year String,
                    age String,
                    age_group String,

                    -- –≠–º–∏—Ç–µ–Ω—Ç
                    iss_flag UInt8,
                    emitent_filial String,
                    emitent_region String,
                    emitent_net String,
                    emitent_bank String,
                    emission_country String,

                    -- –≠–∫–≤–∞–π–µ—Ä
                    acq_flag UInt8,
                    acquirer_net String,
                    acquirer_bank String,
                    acquirer_mfo String,
                    acquirer_branch String,
                    acquirer_region String,

                    -- –ú–µ—Ä—á–∞–Ω—Ç
                    mcc UInt16,
                    merchant_name String,
                    merchant_type String,
                    merchant UInt32,
                    merch_id UInt32,
                    oked Nullable(Float32),
                    terminal_id String,
                    terminal_type String,
                    term_id_key String,
                    address_name String,
                    address_country String,

                    -- IP –∏ –ª–æ–≥–∏–Ω
                    ip String,
                    login_category String,
                    login_group String,
                    login String,

                    -- –°—É–º–º—ã
                    amount_uzs UInt64,
                    reqamt UInt64,
                    conamt UInt64,
                    currency UInt16,

                    -- –°—Ç–∞—Ç—É—Å—ã
                    record_state String,
                    match_num UInt32,
                    reversal_flag UInt8,
                    respcode Nullable(Float32),
                    credit_debit String,
                    data_flag String,
                    trans_type_by_day_key String,

                    -- –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
                    fe_trace UInt32,
                    refnum UInt64,
                    sttl_date UInt32,
                    sttl_hour UInt8,
                    sttl_minute UInt8,
                    hour_num UInt8,
                    minute_num UInt8,
                    udatetime_month UInt8,

                    -- –ò–Ω—Å—Ç–∞–Ω—Å –¥–∞–Ω–Ω—ã–µ
                    inst_id UInt32,
                    inst_id2 UInt32,
                    bo_table String,

                    -- P2P –¥–∞–Ω–Ω—ã–µ
                    p2p_flag UInt8,
                    p2p_type String,
                    sender_hpan String,
                    sender_bank String,
                    receiver_hpan String,
                    receiver_bank String,

                    -- –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø–æ–ª—è
                    inserted_at DateTime DEFAULT now()
                )
                ENGINE = MergeTree()
                PARTITION BY toYYYYMM(toDate(rday))
                ORDER BY (rday, transaction_code, hpan)
                SETTINGS index_granularity = 8192
                """

                self.ch_manager.execute(create_table_sql)
                logger.info("‚úÖ –¢–∞–±–ª–∏—Ü–∞ transactions_main —Å–æ–∑–¥–∞–Ω–∞")

            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏: {e}")
            return False

    def load_data_from_csv(self, file_path: str, batch_size: int = 10000) -> bool:
        """
        –ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV –≤ ClickHouse

        Args:
            file_path: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏

        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞
        """
        file_path = Path(file_path)

        if not file_path.exists():
            logger.error(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False

        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_path}...")

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º CSV
            logger.info("–ß—Ç–µ–Ω–∏–µ CSV —Ñ–∞–π–ª–∞...")
            df = pd.read_csv(file_path, low_memory=False)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏–∑ CSV")

            # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            logger.info("–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç
            date_columns = ['expire_date', 'issue_date']
            for col in date_columns:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col], errors='coerce')
                    df[col] = df[col].fillna(pd.Timestamp('2024-01-01'))

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            numeric_columns = {
                'hpan': 0.0,
                'rday': 0,
                'iss_flag': 0,
                'acq_flag': 0,
                'mcc': 0,
                'amount_uzs': 0,
                'reqamt': 0,
                'conamt': 0,
                'match_num': 0,
                'reversal_flag': 0,
                'fe_trace': 0,
                'refnum': 0,
                'sttl_date': 0,
                'sttl_hour': 0,
                'sttl_minute': 0,
                'hour_num': 0,
                'minute_num': 0,
                'udatetime_month': 0,
                'merchant': 0,
                'currency': 860,  # UZS –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                'merch_id': 0,
                'inst_id': 0,
                'inst_id2': 0,
                'p2p_flag': 0
            }

            for col, default_val in numeric_columns.items():
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(default_val)

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            string_columns = [col for col in df.columns if col not in numeric_columns and col not in date_columns]
            for col in string_columns:
                if col in df.columns:
                    df[col] = df[col].fillna('').astype(str).str.strip()
                    df[col] = df[col].replace('nan', '')

            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ –ø–æ–ª–µ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if 'inserted_at' not in df.columns:
                df['inserted_at'] = datetime.now()

            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –ë–î
            self.ch_manager.execute("USE card_analytics")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞–º–∏
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ ClickHouse –±–∞—Ç—á–∞–º–∏ –ø–æ {batch_size}...")

            total_loaded = 0
            for i in range(0, len(df), batch_size):
                batch = df.iloc[i:i + batch_size]

                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º insert_df –∏–∑ clickhouse_client
                    self.ch_manager.insert_df('transactions_main', batch, batch_size=batch_size)
                    total_loaded += len(batch)
                    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {total_loaded}/{len(df)} –∑–∞–ø–∏—Å–µ–π...")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –±–∞—Ç—á–∞: {e}")
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –±–∞—Ç—á–µ–π
                    continue

            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {total_loaded} –∑–∞–ø–∏—Å–µ–π –≤ transactions_main")
            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False

    def verify_tables(self) -> Dict[str, Any]:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–∞–±–ª–∏—Ü–∞—Ö
        """
        logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∞–±–ª–∏—Ü...")

        try:
            self.ch_manager.execute("USE card_analytics")

            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü
            tables = self.ch_manager.query("SHOW TABLES")
            table_names = [t[0] for t in tables]

            table_info = {}
            for table_name in table_names:
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
                count = self.ch_manager.query(f"SELECT count() FROM {table_name}")[0][0]

                # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ç–∞–±–ª–∏—Ü—ã
                size_query = f"""
                SELECT 
                    formatReadableSize(sum(bytes_on_disk)) as size
                FROM system.parts
                WHERE database = 'card_analytics' 
                    AND table = '{table_name}'
                    AND active
                """
                size_result = self.ch_manager.query(size_query)
                size = size_result[0][0] if size_result else '0 B'

                table_info[table_name] = {
                    'records': count,
                    'size': size
                }

                logger.info(f"  üìä {table_name}: {count:,} –∑–∞–ø–∏—Å–µ–π, —Ä–∞–∑–º–µ—Ä: {size}")

            return table_info

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ç–∞–±–ª–∏—Ü: {e}")
            return {}

    def print_summary(self):
        """–í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        print("\n" + "=" * 60)
        print("  CARD ANALYTICS PLATFORM - –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
        print("=" * 60)

        table_info = self.verify_tables()

        if table_info:
            print("\nüìä –°–¢–ê–¢–£–° –¢–ê–ë–õ–ò–¶:")
            print("-" * 40)
            total_records = 0
            for table, info in table_info.items():
                print(f"  ‚Ä¢ {table}: {info['records']:,} –∑–∞–ø–∏—Å–µ–π ({info['size']})")
                if table == 'transactions_main':
                    total_records = info['records']

            if total_records > 0:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
                print("\nüîç –ü–†–ò–ú–ï–†–´ –ó–ê–ü–†–û–°–û–í –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:")
                print("-" * 40)
                print("  -- –¢–æ–ø MCC –∫–∞—Ç–µ–≥–æ—Ä–∏–π:")
                print("  SELECT mcc, count() as cnt")
                print("  FROM card_analytics.transactions_main")
                print("  GROUP BY mcc ORDER BY cnt DESC LIMIT 10;")
                print()
                print("  -- P2P —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
                print("  SELECT p2p_flag, count(), avg(amount_uzs)")
                print("  FROM card_analytics.transactions_main")
                print("  GROUP BY p2p_flag;")

        print("\nüöÄ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
        print("-" * 40)
        print("  1. –ó–∞–ø—É—Å—Ç–∏—Ç—å Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:")
        print("     streamlit run run_app.py")
        print()
        print("  2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ ClickHouse:")
        print("     http://localhost:8123/play")
        print()
        print("  3. –û—Ç–∫—Ä—ã—Ç—å Jupyter –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:")
        print("     http://localhost:8888")
        print()
        print("=" * 60)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse

    parser = argparse.ArgumentParser(description='–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î Card Analytics Platform')
    parser.add_argument('--skip-migrations', action='store_true', help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –º–∏–≥—Ä–∞—Ü–∏–∏')
    parser.add_argument('--file', type=str, default='data_100k.csv',
                        help='CSV —Ñ–∞–π–ª –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é data_100k.csv)')
    parser.add_argument('--batch-size', type=int, default=10000,
                        help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10000)')

    args = parser.parse_args()

    # –°–æ–∑–¥–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ç–æ—Ä
    initializer = DatabaseInitializer()

    # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    print("\nüîå –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ ClickHouse...")
    if not initializer.check_connection():
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ ClickHouse!")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω:")
        print("  docker-compose up -d")
        sys.exit(1)

    # –®–∞–≥ 2: –ó–∞–ø—É—Å–∫ –º–∏–≥—Ä–∞—Ü–∏–π (—Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü)
    if not args.skip_migrations:
        print("\nüìù –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü...")
        if not initializer.run_migrations():
            print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü")
            sys.exit(1)

    # –®–∞–≥ 3: –ü–æ–∏—Å–∫ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    data_files_priority = [
        args.file,  # –£–∫–∞–∑–∞–Ω–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º —Ñ–∞–π–ª
        'data_100k.csv',  # –§–∞–π–ª —Å 100–∫ –∑–∞–ø–∏—Å–µ–π
        'output.csv',  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª —Å 50 –∑–∞–ø–∏—Å—è–º–∏
        'data/data_100k.csv',  # –í –ø–∞–ø–∫–µ data
        'data/output.csv'
    ]

    data_loaded = False
    for data_file in data_files_priority:
        if data_file and Path(data_file).exists():
            file_size = Path(data_file).stat().st_size / 1024 / 1024  # –í –ú–ë
            print(f"\nüìä –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {data_file} ({file_size:.2f} MB)")

            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
            with open(data_file, 'r', encoding='utf-8') as f:
                first_line = f.readline()
                sample_lines = f.read(1024 * 100)  # –ß–∏—Ç–∞–µ–º 100KB –¥–ª—è –æ—Ü–µ–Ω–∫–∏
                lines_in_sample = sample_lines.count('\n')
                estimated_lines = int(lines_in_sample * file_size * 1024 / 100)
                print(f"   –ü—Ä–∏–º–µ—Ä–Ω–æ {estimated_lines:,} —Å—Ç—Ä–æ–∫")

            print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_file}...")
            if initializer.load_data_from_csv(data_file, batch_size=args.batch_size):
                data_loaded = True
                break

    if not data_loaded:
        print("\n‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏!")
        print("–ü–æ–º–µ—Å—Ç–∏—Ç–µ —Ñ–∞–π–ª data_100k.csv –≤ –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞")
        sys.exit(1)

    # –®–∞–≥ 4: –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    initializer.print_summary()


if __name__ == "__main__":
    main()